<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reconhecimento Facial</title>
    <script src="face-api.js"></script> <!-- Carregando a biblioteca localmente -->
    <style>
        canvas {
            position: absolute;
            top: 0;
            left: 0;
        }
    </style>
</head>
<body>
    <h1>Reconhecimento Facial</h1>
    <select id="cameraSelect" style="color: #faebd700; border: azure;"></select>
    <video id="video" width="720" height="560" autoplay></video>
    <canvas id="overlay" width="720" height="560" style="position: absolute; top: 0; left: 0;"></canvas>
    <script>
        const video = document.getElementById('video');
        const cameraSelect = document.getElementById('cameraSelect');
        const overlay = document.getElementById('overlay');
        const context = overlay.getContext('2d');

        // Carregar os modelos
        async function loadModels() {
            faceapi.tf.setBackend('cpu'); // Usar CPU se WebGL não estiver disponível
            await faceapi.nets.tinyFaceDetector.loadFromUri('models');
            await faceapi.nets.faceLandmark68Net.loadFromUri('models');
            await faceapi.nets.faceRecognitionNet.loadFromUri('models');
        }

        // Iniciar o vídeo
        async function startVideo(deviceId) {
            const constraints = {
                video: { deviceId: deviceId ? { exact: deviceId } : undefined }
            };
            const stream = await navigator.mediaDevices.getUserMedia(constraints);
            video.srcObject = stream;
        }

        // Listar câmeras disponíveis
        async function listCameras() {
            const devices = await navigator.mediaDevices.enumerateDevices();
            const cameras = devices.filter(device => device.kind === 'videoinput');

            cameraSelect.innerHTML = ''; // Limpar opções

            cameras.forEach(camera => {
                const option = document.createElement('option');
                option.value = camera.deviceId;
                option.textContent = camera.label || `Câmera ${camera.deviceId}`;
                cameraSelect.appendChild(option);
            });

            // Iniciar com a primeira câmera
            if (cameras.length > 1) {
                await startVideo(cameras[1].deviceId);
            }
        }

        // Detectar faces
        async function detectFace() {
            const result = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions());
            context.clearRect(0, 0, overlay.width, overlay.height); // Limpar canvas

            if (result) {
                const resizedResult = faceapi.resizeResults(result, { width: video.width, height: video.height });
                faceapi.draw.drawDetections(overlay, resizedResult);
                context.fillStyle = "green"; // Cor quando o rosto é detectado
                context.font = "20px Arial";
                context.fillText("Rosto Detectado!", 10, 20);
            }

            requestAnimationFrame(detectFace);
        }

        // Inicializar
        (async () => {
            await loadModels();
            await listCameras();
            cameraSelect.addEventListener('change', async () => {
                await startVideo(cameraSelect.value);
            });
            detectFace();
        })();
    </script>
</body>
</html>
